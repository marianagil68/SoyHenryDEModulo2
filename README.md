#  Proyecto Integrador â€“ Data Engineering: OptimizaciÃ³n de Plataforma E-Commerce: Data Staging & Exploration

El primer avance contiene la primera etapa del desarrollo de una plataforma de datos para un sistema de ecommerce. El objetivo es construir la capa de **staging**, realizar la carga inicial desde archivos CSV, ejecutar un proceso de perfilamiento de datos y establecer las bases para futuras transformaciones y modelado analÃ­tico.

---
# PI 1 - ConfiguraciÃ³n del entorno de trabajo
##  Arquitectura del entorno

###  Base de datos (Docker + PostgreSQL)

El sistema utiliza **PostgreSQL** desplegado en un contenedor Docker, garantizando portabilidad y aislamiento del entorno.

###  AdministraciÃ³n de la base

**pgAdmin 4** instalado en el host local se utiliza para la administraciÃ³n visual del servidor PostgreSQL.

###  ConexiÃ³n desde Python

El proyecto implementa un mÃ³dulo de conexiÃ³n mediante **SQLAlchemy**, configurado con variables de entorno.

Las credenciales se almacenan en el archivo `.env`:

```
PG_HOST=localhost
PG_PORT=5432
PG_USER=postgres
PG_PASSWORD=<password>
PG_DB=ecommerce
```

---

## ğŸ“‚ Estructura del proyecto

```
project/
â”‚â”€â”€ data/                                               # Archivos CSV originales
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ connection.py                               # Engine + Session
â”‚   â”‚   â”œâ”€â”€ models.py                                   # Tablas staging (sin FK)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ loader.py                                   # Proceso de ingestiÃ³n inicial
â”‚   â”‚   â”œâ”€â”€ csv_factory.py
â”‚   â””â”€â”€ Avance1     /                                   # AnÃ¡lisis exploratorio
â”‚       â””â”€â”€ Avance1_Pi1_2_ORM_CreacionDeTablasYCarga.ipynb
â”‚       â””â”€â”€ Avence1_Pi3_CamposSemiEstructurados.iypnb
â”‚       â””â”€â”€ Avence1_Pi4_CalidadDeLosDatos.iypnb
â”‚       â””â”€â”€ Avence1_Pi4__modelo_datos.ipynb
â”‚       â””â”€â”€ Avence1_Pi5__ReporteDeHallazgos.ipynb
â”‚   â””â”€â”€ Avance2     /                                   # Mejora de detos, modelos, SCD y KPI
â”‚       â””â”€â”€ Hw0_MejorarLosDatosEnEsquemaProduction.sql
â”‚       â””â”€â”€ Hw1_KPI.ipynb
â”‚       â””â”€â”€ hw1_PreguntasDeNegocio.ipynb
â”‚       â””â”€â”€ hw1_PreguntasDeNegocio.sql
â”‚       â””â”€â”€ Hw2_Documentacion.pdf
â”‚       â””â”€â”€ Hw2_ModeloConceptual.jpg
â”‚       â””â”€â”€ Hw2_ModeloLogico.png
â”‚       â””â”€â”€ Hw3_4_5_DiagramaHechosDimensiones.png
â”‚       â””â”€â”€ Hw2_ModeloFisico.png
â”‚       â””â”€â”€ Hw7_DocumentacionFinalConSCD.pdf
â”‚   â””â”€â”€ Avance3     /                                   # Desarrollo de DBT
â”‚       â””â”€â”€ dbt
â”‚           â””â”€â”€ models
â”‚               â””â”€â”€ intermediate
â”‚               â””â”€â”€ marts
â”‚               â””â”€â”€ staging
â”‚           â””â”€â”€ target
â”‚       â””â”€â”€ docker-compose.yml
â”‚       â””â”€â”€ Dockerfile
â”‚â”€â”€ .env
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
```

---

## âš™ï¸ ConfiguraciÃ³n y despliegue

### 1. Levantar la base de datos con Docker

```bash
docker-compose up -d
```

Esto despliega:

* Servidor PostgreSQL
* Volumen persistente
* Puerto expuesto en `5432`

### 2. Crear entorno virtual e instalar dependencias

```bash
python -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Configurar conexiÃ³n (ORM)

La conexiÃ³n se gestiona desde `src/database/connection.py` usando SQLAlchemy.

```python
engine = DB.engine()
SessionLocal = DB.session()
```

---
# PI2 - Carga inicial de datos
## Ingesta inicial de datos (Staging Layer)

La capa de staging permite cargar datos crudos provenientes de archivos CSV sin validaciÃ³n de integridad referencial.

### CreaciÃ³n de tablas

```python
from src.database.models import Base
from src.database.connection import DB
Base.metadata.create_all(DB.engine())
```

### Proceso de carga

```python
from src.services.loader import load_all
load_all()
```

### Registros cargados

| Tabla           | Filas |
| --------------- | ----- |
| usuarios        | 1000  |
| categorias      | 12    |
| productos       | 36    |
| ordenes         | 10000 |
| detalle_ordenes | 10000 |

---
# PI3 - Tratamiento de campos semiestructurados
## RevisiÃ³n del contenido de las tablas

La capa de staging permite cargar datos crudos provenientes de archivos CSV sin validaciÃ³n de integridad referencial.


---

## ExploraciÃ³n y evaluaciÃ³n de calidad de datos

El anÃ¡lisis exploratorio combina consultas SQL y procesamiento en Python.

### Se evaluaron los siguientes aspectos:

* Valores nulos
* Duplicados
* Inconsistencias entre entidades
* Campos con formato no estÃ¡ndar o semi-estructurado
* Atributos clave y dependencias entre tablas

Los anÃ¡lisis se encuentran en:

```
src/exploration/
```

---


## Preguntas de negocio

### Ventas

* Productos mÃ¡s vendidos por volumen
* Ticket promedio
* CategorÃ­as con mayor volumen de ventas
* Actividad por dÃ­a de la semana
* VariaciÃ³n mensual de Ã³rdenes

### Pagos

* MÃ©todos de pago mÃ¡s utilizados
* Monto promedio por mÃ©todo
* Ã“rdenes con mÃºltiples mÃ©todos
* Pagos con estado fallido o procesando
* RecaudaciÃ³n mensual

### Usuarios

* Altas de usuarios por mes
* Usuarios con mÃºltiples compras
* Usuarios registrados sin compras
* Usuarios con mayor gasto acumulado
* Usuarios que dejaron reseÃ±as

### Productos y stock

* Productos con alto stock y bajas ventas
* Productos sin stock
* Productos peor calificados
* Productos con mÃ¡s reseÃ±as
* CategorÃ­as con mayor valor econÃ³mico vendido

---

## Avance 2 â€“ Modelado, Mejora de Datos, SCD y KPIs

### Mejora de Datos en el Esquema Production
Durante este avance se implementaron procesos de estandarizaciÃ³n, limpieza y enriquecimiento de datos dentro del esquema `production`. Entre las mejoras realizadas se incluyen:

- NormalizaciÃ³n de formatos de texto y fechas.
- CorrecciÃ³n de claves inconsistentes entre entidades.
- DeduplicaciÃ³n y control de integridad.
- TransformaciÃ³n y validaciÃ³n de campos numÃ©ricos y categÃ³ricos.
- Ajustes derivados del anÃ¡lisis de calidad del avance 1.

El desarrollo se encuentra en:

```
src/Avance2/Hw0_MejorarLosDatosEnEsquemaProduction.sql
```

### Modelado Conceptual, LÃ³gico y FÃ­sico
Se desarrollaron los modelos fundamentales para soportar el futuro Data Warehouse:

- Modelo conceptual: `Hw2_ModeloConceptual.jpg`
- Modelo lÃ³gico: `Hw2_ModeloLogico.png`
- Modelo fÃ­sico: `Hw2_ModeloFisico.png`

Estos modelos definen entidades, relaciones, cardinalidades y reglas del negocio, y sirven como base para el diseÃ±o dimensional.

### ImplementaciÃ³n de Slowly Changing Dimensions (SCD)
Se definiÃ³ e implementÃ³ la estrategia de SCD Tipo 2 para dimensiones como usuarios, productos y categorÃ­as, preservando la trazabilidad histÃ³rica de cambios.

La documentaciÃ³n correspondiente se encuentra en:

```
src/Avance2/Hw7_DocumentacionFinalConSCD.pdf
```

### KPIs y Preguntas de Negocio
Se desarrollaron mÃ©tricas y anÃ¡lisis orientados al negocio, incluyendo ventas, recaudaciÃ³n, ticket promedio, actividad de usuarios y comportamiento por categorÃ­a y producto.

Los materiales generados se encuentran en:

```
src/Avance2/Hw1_KPI.ipynb
src/Avance2/hw1_PreguntasDeNegocio.ipynb
src/Avance2/hw1_PreguntasDeNegocio.sql
src/Avance2/Hw2_Documentacion.pdf
```

---

## Avance 3 â€“ Desarrollo del Proyecto con DBT

### Estructura del Proyecto DBT
Se construyÃ³ un proyecto DBT completo siguiendo un enfoque por capas:

```
src/Avance3/dbt/
â”‚â”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”œâ”€â”€ intermediate/
â”‚   â””â”€â”€ marts/
â”‚â”€â”€ target/
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ Dockerfile
```

### Modelos Implementados

#### Modelos de staging
- Limpieza y estandarizaciÃ³n de columnas.
- TipificaciÃ³n de datos.
- Renombrado siguiendo convenciones DBT.

#### Modelos intermediate
- Transformaciones intermedias.
- Enriquecimiento de entidades.
- CÃ¡lculo de campos derivados.

#### Modelos marts (modelo dimensional)
- Tablas de hechos: ventas, detalle de Ã³rdenes, pagos.
- Dimensiones: productos, usuarios, categorÃ­as, fechas.
- IncorporaciÃ³n de SCD donde corresponde.

### Despliegue y EjecuciÃ³n con Docker
Para garantizar un entorno reproducible se configuraron contenedores Docker:

- `Dockerfile` para la imagen de DBT.
- `docker-compose.yml` para ejecutar DBT junto con PostgreSQL.



### Autor
**Mariana Gil**  
Data Engineer | Proyecto Integrador â€“ Curso de Data Engineering  
[LinkedIn](https://www.linkedin.com/in/mariana-gil-24667718/) Â· [GitHub](https://github.com/marianagil68/SoyHenryDEModulo2)